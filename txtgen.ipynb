{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f84b5c0c-9689-4e42-ada2-3f375fd9c956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"C:/Users/bhudi/Desktop/New folder/git/txtgen/DL.txt\",'r',encoding='utf-8') as file:\n",
    "    txt=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1610a771-68af-4760-839a-86e7a0af8bcc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a subset of machine learning that is inspired by the structure and function of the human brain. It involves the use of artificial neural networks, which are computational models that mimic the way neurons in the brain process information. Deep learning algorithms are designed to learn from large amounts of data, extracting patterns and features that can be used for a variety of tasks, such as image recognition, natural language processing, and decision-making.\n",
      "At the core of deep learning are neural networks, which are composed of interconnected nodes, called neurons, that are organized into layers. These layers can be broadly categorized into three types: input layers, hidden layers, and output layers. The input layer receives the raw data, such as an image or text, and passes it to the hidden layers. The hidden layers perform transformations on the data, extracting features and patterns that are relevant to the task at hand. The output layer produces the final result, such as a classification or a prediction.\n",
      "One of the key advantages of deep learning is its ability to automatically learn features from data, without the need for manual feature engineering. Traditional machine learning algorithms typically rely on hand-crafted features, which can be time-consuming and require significant domain expertise. In contrast, deep learning algorithms can learn to extract relevant features directly from the raw data, making them more flexible and adaptable to various tasks and domains.\n",
      "Deep learning models are trained using a process called backpropagation, which involves adjusting the weights of the connections between neurons in the neural network based on the difference between the predicted output and the desired output. This process is repeated iteratively, allowing the model to gradually improve its performance over time.\n",
      "There are several types of neural networks used in deep learning, each with its own strengths and applications. Convolutional Neural Networks (CNNs) are particularly well-suited for processing image data, as they can effectively capture spatial and temporal dependencies. Recurrent Neural Networks (RNNs) are designed to handle sequential data, such as text or speech, and can model long-term dependencies in the data. Long Short-Term Memory (LSTM) networks are a type of RNN that addresses the problem of vanishing gradients, which can occur when training deep neural networks.\n",
      "Deep learning has revolutionized many fields, including computer vision, natural language processing, and speech recognition. In computer vision, deep learning models have achieved remarkable performance in tasks such as object detection, image classification, and image segmentation. For example, CNNs have been used to develop highly accurate systems for recognizing objects in images, with applications ranging from self-driving cars to medical imaging analysis.\n",
      "In natural language processing, deep learning has enabled significant advances in tasks such as machine translation, sentiment analysis, and text generation. Recurrent Neural Networks (RNNs) and attention-based models, such as the Transformer architecture, have been particularly successful in these areas, allowing for more accurate and nuanced language understanding and generation.\n",
      "Deep learning has also had a profound impact on speech recognition and synthesis. Deep Neural Networks (DNNs) and Recurrent Neural Networks (RNNs) have been used to develop highly accurate speech recognition systems that can transcribe speech in real-time and handle diverse accents and environments. Similarly, deep learning has enabled the generation of highly natural-sounding synthetic speech, with applications in virtual assistants, audiobook narration, and accessibility technologies.\n",
      "Despite its many successes, deep learning is not without its challenges and limitations. One major challenge is the need for large amounts of labeled data for training the models. Obtaining and annotating high-quality datasets can be time-consuming and expensive, particularly in domains where expert knowledge is required. Another challenge is the interpretability of deep learning models, as it can be difficult to understand the reasoning behind the decisions made by these complex models.\n",
      "To address these challenges, researchers have developed various techniques and approaches. Transfer learning allows models trained on one task or domain to be fine-tuned for a different task or domain, reducing the need for large amounts of labeled data. Explainable AI (XAI) aims to develop methods for making deep learning models more interpretable and transparent, helping to build trust and accountability in these systems.\n",
      "Another important area of research in deep learning is the development of more efficient and scalable algorithms and architectures. As deep learning models become larger and more complex, there is a need for computational resources and energy-efficient hardware solutions. Techniques such as model compression, quantization, and hardware acceleration have been explored to address these challenges.\n",
      "Deep learning has also been combined with other machine learning techniques, such as reinforcement learning and generative adversarial networks (GANs), to tackle more complex problems and create more powerful models. Reinforcement learning leverages deep neural networks to learn optimal decision-making strategies through interaction with an environment, enabling applications in areas such as robotics and game playing. GANs, on the other hand, involve training two neural networks simultaneously, with one network generating synthetic data and the other discriminating between real and generated data. This approach has been used for tasks such as image generation, style transfer, and data augmentation.\n",
      "Despite the remarkable progress in deep learning, there are still many open challenges and areas for future research. One major challenge is developing deep learning models that can adapt and generalize to new situations and domains, rather than being narrowly specialized for a specific task. Another challenge is creating models that can learn from limited or incomplete data, mimicking the human ability to learn from sparse and noisy information.\n",
      "There is also a growing interest in developing more efficient and sustainable deep learning models, with a focus on reducing the computational and energy requirements of these systems. This includes exploring new hardware architectures, optimizing existing models, and developing new training algorithms that are more efficient and scalable.\n",
      "Another area of active research is the intersection of deep learning and other fields, such as neuroscience and cognitive science. By studying the mechanisms of the human brain and how it processes information, researchers hope to gain insights that can inspire new deep learning architectures and algorithms.\n",
      "As deep learning continues to evolve and advance, its impact on various industries and domains is expected to grow. Deep learning has already been integrated into a wide range of applications, from virtual assistants and recommendation systems to medical diagnosis and scientific research. As the technology matures and becomes more accessible, it is likely that deep learning will become increasingly ubiquitous, enabling new capabilities and transforming the way we interact with and understand the world around us.\n",
      "However, the widespread adoption of deep learning also raises important ethical and societal considerations. Issues such as privacy, fairness, and accountability will need to be carefully addressed as these powerful technologies are deployed in real-world settings. There is a need for robust governance frameworks and ethical guidelines to ensure that deep learning is developed and used in a responsible and equitable manner.\n",
      "In conclusion, deep learning is a rapidly evolving and transformative field that has already made significant contributions to various domains, from computer vision and natural language processing to robotics and scientific research. While there are still many challenges and limitations to overcome, the potential of deep learning to revolutionize the way we interact with and understand the world is immense. As researchers continue to push the boundaries of this technology, it will be important to address the ethical and societal implications, ensuring that deep learning is developed and deployed in a responsible and equitable manner.\n"
     ]
    }
   ],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f72e0c-f287-47d0-9790-a549af422833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3a1130-0221-4e1c-b722-ecd8c041f6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tk=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fc17ef5-689e-44a5-9e0a-af3f23618826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tk.fit_on_texts([txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accb7de9-9187-475d-8ef5-f26aedbe521f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1,\n",
       " 'the': 2,\n",
       " 'learning': 3,\n",
       " 'deep': 4,\n",
       " 'to': 5,\n",
       " 'of': 6,\n",
       " 'in': 7,\n",
       " 'is': 8,\n",
       " 'as': 9,\n",
       " 'a': 10,\n",
       " 'for': 11,\n",
       " 'that': 12,\n",
       " 'are': 13,\n",
       " 'models': 14,\n",
       " 'data': 15,\n",
       " 'networks': 16,\n",
       " 'can': 17,\n",
       " 'such': 18,\n",
       " 'neural': 19,\n",
       " 'more': 20,\n",
       " 'with': 21,\n",
       " 'from': 22,\n",
       " 'be': 23,\n",
       " 'these': 24,\n",
       " 'on': 25,\n",
       " 'has': 26,\n",
       " 'layers': 27,\n",
       " 'been': 28,\n",
       " 'it': 29,\n",
       " 'algorithms': 30,\n",
       " 'learn': 31,\n",
       " 'used': 32,\n",
       " 'image': 33,\n",
       " 'or': 34,\n",
       " 'its': 35,\n",
       " 'need': 36,\n",
       " 'there': 37,\n",
       " 'speech': 38,\n",
       " 'have': 39,\n",
       " 'which': 40,\n",
       " 'features': 41,\n",
       " 'tasks': 42,\n",
       " 'natural': 43,\n",
       " 'language': 44,\n",
       " 'processing': 45,\n",
       " 'one': 46,\n",
       " 'domains': 47,\n",
       " 'applications': 48,\n",
       " 'systems': 49,\n",
       " 'challenges': 50,\n",
       " 'research': 51,\n",
       " 'new': 52,\n",
       " 'machine': 53,\n",
       " 'recognition': 54,\n",
       " 'making': 55,\n",
       " 'output': 56,\n",
       " 'task': 57,\n",
       " 'time': 58,\n",
       " 'various': 59,\n",
       " 'this': 60,\n",
       " 'training': 61,\n",
       " 'many': 62,\n",
       " 'generation': 63,\n",
       " 'also': 64,\n",
       " 'challenge': 65,\n",
       " 'another': 66,\n",
       " 'efficient': 67,\n",
       " 'other': 68,\n",
       " 'by': 69,\n",
       " 'human': 70,\n",
       " 'brain': 71,\n",
       " 'computational': 72,\n",
       " 'way': 73,\n",
       " 'neurons': 74,\n",
       " 'process': 75,\n",
       " 'information': 76,\n",
       " 'large': 77,\n",
       " 'amounts': 78,\n",
       " 'into': 79,\n",
       " 'hidden': 80,\n",
       " 'text': 81,\n",
       " 'hand': 82,\n",
       " 'significant': 83,\n",
       " 'domain': 84,\n",
       " 'between': 85,\n",
       " 'model': 86,\n",
       " 'particularly': 87,\n",
       " 'recurrent': 88,\n",
       " 'rnns': 89,\n",
       " 'computer': 90,\n",
       " 'vision': 91,\n",
       " 'develop': 92,\n",
       " 'highly': 93,\n",
       " 'accurate': 94,\n",
       " 'areas': 95,\n",
       " 'real': 96,\n",
       " 'understand': 97,\n",
       " 'complex': 98,\n",
       " 'address': 99,\n",
       " 'researchers': 100,\n",
       " 'developed': 101,\n",
       " 'techniques': 102,\n",
       " 'important': 103,\n",
       " 'architectures': 104,\n",
       " 'hardware': 105,\n",
       " 'developing': 106,\n",
       " 'will': 107,\n",
       " 'world': 108,\n",
       " 'ethical': 109,\n",
       " 'involves': 110,\n",
       " 'designed': 111,\n",
       " 'extracting': 112,\n",
       " 'patterns': 113,\n",
       " 'decision': 114,\n",
       " 'at': 115,\n",
       " 'called': 116,\n",
       " 'types': 117,\n",
       " 'input': 118,\n",
       " 'layer': 119,\n",
       " 'raw': 120,\n",
       " 'an': 121,\n",
       " 'relevant': 122,\n",
       " 'classification': 123,\n",
       " 'ability': 124,\n",
       " 'without': 125,\n",
       " 'consuming': 126,\n",
       " 'trained': 127,\n",
       " 'network': 128,\n",
       " 'based': 129,\n",
       " 'allowing': 130,\n",
       " 'performance': 131,\n",
       " 'cnns': 132,\n",
       " 'dependencies': 133,\n",
       " 'handle': 134,\n",
       " 'long': 135,\n",
       " 'term': 136,\n",
       " 'fields': 137,\n",
       " 'remarkable': 138,\n",
       " 'medical': 139,\n",
       " 'analysis': 140,\n",
       " 'enabled': 141,\n",
       " 'impact': 142,\n",
       " 'synthetic': 143,\n",
       " 'virtual': 144,\n",
       " 'assistants': 145,\n",
       " 'technologies': 146,\n",
       " 'despite': 147,\n",
       " 'limitations': 148,\n",
       " 'major': 149,\n",
       " 'labeled': 150,\n",
       " 'made': 151,\n",
       " 'transfer': 152,\n",
       " 'reducing': 153,\n",
       " 'accountability': 154,\n",
       " 'area': 155,\n",
       " 'scalable': 156,\n",
       " 'become': 157,\n",
       " 'energy': 158,\n",
       " 'reinforcement': 159,\n",
       " 'gans': 160,\n",
       " 'powerful': 161,\n",
       " 'enabling': 162,\n",
       " 'robotics': 163,\n",
       " 'still': 164,\n",
       " 'already': 165,\n",
       " 'scientific': 166,\n",
       " 'technology': 167,\n",
       " 'we': 168,\n",
       " 'interact': 169,\n",
       " 'societal': 170,\n",
       " 'deployed': 171,\n",
       " 'responsible': 172,\n",
       " 'equitable': 173,\n",
       " 'manner': 174,\n",
       " 'subset': 175,\n",
       " 'inspired': 176,\n",
       " 'structure': 177,\n",
       " 'function': 178,\n",
       " 'use': 179,\n",
       " 'artificial': 180,\n",
       " 'mimic': 181,\n",
       " 'variety': 182,\n",
       " 'core': 183,\n",
       " 'composed': 184,\n",
       " 'interconnected': 185,\n",
       " 'nodes': 186,\n",
       " 'organized': 187,\n",
       " 'broadly': 188,\n",
       " 'categorized': 189,\n",
       " 'three': 190,\n",
       " 'receives': 191,\n",
       " 'passes': 192,\n",
       " 'perform': 193,\n",
       " 'transformations': 194,\n",
       " 'produces': 195,\n",
       " 'final': 196,\n",
       " 'result': 197,\n",
       " 'prediction': 198,\n",
       " 'key': 199,\n",
       " 'advantages': 200,\n",
       " 'automatically': 201,\n",
       " 'manual': 202,\n",
       " 'feature': 203,\n",
       " 'engineering': 204,\n",
       " 'traditional': 205,\n",
       " 'typically': 206,\n",
       " 'rely': 207,\n",
       " 'crafted': 208,\n",
       " 'require': 209,\n",
       " 'expertise': 210,\n",
       " 'contrast': 211,\n",
       " 'extract': 212,\n",
       " 'directly': 213,\n",
       " 'them': 214,\n",
       " 'flexible': 215,\n",
       " 'adaptable': 216,\n",
       " 'using': 217,\n",
       " 'backpropagation': 218,\n",
       " 'adjusting': 219,\n",
       " 'weights': 220,\n",
       " 'connections': 221,\n",
       " 'difference': 222,\n",
       " 'predicted': 223,\n",
       " 'desired': 224,\n",
       " 'repeated': 225,\n",
       " 'iteratively': 226,\n",
       " 'gradually': 227,\n",
       " 'improve': 228,\n",
       " 'over': 229,\n",
       " 'several': 230,\n",
       " 'each': 231,\n",
       " 'own': 232,\n",
       " 'strengths': 233,\n",
       " 'convolutional': 234,\n",
       " 'well': 235,\n",
       " 'suited': 236,\n",
       " 'they': 237,\n",
       " 'effectively': 238,\n",
       " 'capture': 239,\n",
       " 'spatial': 240,\n",
       " 'temporal': 241,\n",
       " 'sequential': 242,\n",
       " 'short': 243,\n",
       " 'memory': 244,\n",
       " 'lstm': 245,\n",
       " 'type': 246,\n",
       " 'rnn': 247,\n",
       " 'addresses': 248,\n",
       " 'problem': 249,\n",
       " 'vanishing': 250,\n",
       " 'gradients': 251,\n",
       " 'occur': 252,\n",
       " 'when': 253,\n",
       " 'revolutionized': 254,\n",
       " 'including': 255,\n",
       " 'achieved': 256,\n",
       " 'object': 257,\n",
       " 'detection': 258,\n",
       " 'segmentation': 259,\n",
       " 'example': 260,\n",
       " 'recognizing': 261,\n",
       " 'objects': 262,\n",
       " 'images': 263,\n",
       " 'ranging': 264,\n",
       " 'self': 265,\n",
       " 'driving': 266,\n",
       " 'cars': 267,\n",
       " 'imaging': 268,\n",
       " 'advances': 269,\n",
       " 'translation': 270,\n",
       " 'sentiment': 271,\n",
       " 'attention': 272,\n",
       " 'transformer': 273,\n",
       " 'architecture': 274,\n",
       " 'successful': 275,\n",
       " 'nuanced': 276,\n",
       " 'understanding': 277,\n",
       " 'had': 278,\n",
       " 'profound': 279,\n",
       " 'synthesis': 280,\n",
       " 'dnns': 281,\n",
       " 'transcribe': 282,\n",
       " 'diverse': 283,\n",
       " 'accents': 284,\n",
       " 'environments': 285,\n",
       " 'similarly': 286,\n",
       " 'sounding': 287,\n",
       " 'audiobook': 288,\n",
       " 'narration': 289,\n",
       " 'accessibility': 290,\n",
       " 'successes': 291,\n",
       " 'not': 292,\n",
       " 'obtaining': 293,\n",
       " 'annotating': 294,\n",
       " 'high': 295,\n",
       " 'quality': 296,\n",
       " 'datasets': 297,\n",
       " 'expensive': 298,\n",
       " 'where': 299,\n",
       " 'expert': 300,\n",
       " 'knowledge': 301,\n",
       " 'required': 302,\n",
       " 'interpretability': 303,\n",
       " 'difficult': 304,\n",
       " 'reasoning': 305,\n",
       " 'behind': 306,\n",
       " 'decisions': 307,\n",
       " 'approaches': 308,\n",
       " 'allows': 309,\n",
       " 'fine': 310,\n",
       " 'tuned': 311,\n",
       " 'different': 312,\n",
       " 'explainable': 313,\n",
       " 'ai': 314,\n",
       " 'xai': 315,\n",
       " 'aims': 316,\n",
       " 'methods': 317,\n",
       " 'interpretable': 318,\n",
       " 'transparent': 319,\n",
       " 'helping': 320,\n",
       " 'build': 321,\n",
       " 'trust': 322,\n",
       " 'development': 323,\n",
       " 'larger': 324,\n",
       " 'resources': 325,\n",
       " 'solutions': 326,\n",
       " 'compression': 327,\n",
       " 'quantization': 328,\n",
       " 'acceleration': 329,\n",
       " 'explored': 330,\n",
       " 'combined': 331,\n",
       " 'generative': 332,\n",
       " 'adversarial': 333,\n",
       " 'tackle': 334,\n",
       " 'problems': 335,\n",
       " 'create': 336,\n",
       " 'leverages': 337,\n",
       " 'optimal': 338,\n",
       " 'strategies': 339,\n",
       " 'through': 340,\n",
       " 'interaction': 341,\n",
       " 'environment': 342,\n",
       " 'game': 343,\n",
       " 'playing': 344,\n",
       " 'involve': 345,\n",
       " 'two': 346,\n",
       " 'simultaneously': 347,\n",
       " 'generating': 348,\n",
       " 'discriminating': 349,\n",
       " 'generated': 350,\n",
       " 'approach': 351,\n",
       " 'style': 352,\n",
       " 'augmentation': 353,\n",
       " 'progress': 354,\n",
       " 'open': 355,\n",
       " 'future': 356,\n",
       " 'adapt': 357,\n",
       " 'generalize': 358,\n",
       " 'situations': 359,\n",
       " 'rather': 360,\n",
       " 'than': 361,\n",
       " 'being': 362,\n",
       " 'narrowly': 363,\n",
       " 'specialized': 364,\n",
       " 'specific': 365,\n",
       " 'creating': 366,\n",
       " 'limited': 367,\n",
       " 'incomplete': 368,\n",
       " 'mimicking': 369,\n",
       " 'sparse': 370,\n",
       " 'noisy': 371,\n",
       " 'growing': 372,\n",
       " 'interest': 373,\n",
       " 'sustainable': 374,\n",
       " 'focus': 375,\n",
       " 'requirements': 376,\n",
       " 'includes': 377,\n",
       " 'exploring': 378,\n",
       " 'optimizing': 379,\n",
       " 'existing': 380,\n",
       " 'active': 381,\n",
       " 'intersection': 382,\n",
       " 'neuroscience': 383,\n",
       " 'cognitive': 384,\n",
       " 'science': 385,\n",
       " 'studying': 386,\n",
       " 'mechanisms': 387,\n",
       " 'how': 388,\n",
       " 'processes': 389,\n",
       " 'hope': 390,\n",
       " 'gain': 391,\n",
       " 'insights': 392,\n",
       " 'inspire': 393,\n",
       " 'continues': 394,\n",
       " 'evolve': 395,\n",
       " 'advance': 396,\n",
       " 'industries': 397,\n",
       " 'expected': 398,\n",
       " 'grow': 399,\n",
       " 'integrated': 400,\n",
       " 'wide': 401,\n",
       " 'range': 402,\n",
       " 'recommendation': 403,\n",
       " 'diagnosis': 404,\n",
       " 'matures': 405,\n",
       " 'becomes': 406,\n",
       " 'accessible': 407,\n",
       " 'likely': 408,\n",
       " 'increasingly': 409,\n",
       " 'ubiquitous': 410,\n",
       " 'capabilities': 411,\n",
       " 'transforming': 412,\n",
       " 'around': 413,\n",
       " 'us': 414,\n",
       " 'however': 415,\n",
       " 'widespread': 416,\n",
       " 'adoption': 417,\n",
       " 'raises': 418,\n",
       " 'considerations': 419,\n",
       " 'issues': 420,\n",
       " 'privacy': 421,\n",
       " 'fairness': 422,\n",
       " 'carefully': 423,\n",
       " 'addressed': 424,\n",
       " 'settings': 425,\n",
       " 'robust': 426,\n",
       " 'governance': 427,\n",
       " 'frameworks': 428,\n",
       " 'guidelines': 429,\n",
       " 'ensure': 430,\n",
       " 'conclusion': 431,\n",
       " 'rapidly': 432,\n",
       " 'evolving': 433,\n",
       " 'transformative': 434,\n",
       " 'field': 435,\n",
       " 'contributions': 436,\n",
       " 'while': 437,\n",
       " 'overcome': 438,\n",
       " 'potential': 439,\n",
       " 'revolutionize': 440,\n",
       " 'immense': 441,\n",
       " 'continue': 442,\n",
       " 'push': 443,\n",
       " 'boundaries': 444,\n",
       " 'implications': 445,\n",
       " 'ensuring': 446}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4916920c-e408-4072-a05a-e35363969487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447\n"
     ]
    }
   ],
   "source": [
    "tot_words=len(tk.word_index)+1\n",
    "print(tot_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d7bdeb-9ded-4238-81c4-860e20c16690",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ip=[]\n",
    "for i in txt.split('\\n'):\n",
    "    tokens=tk.texts_to_sequences([i])[0]\n",
    "    for j in range(1,len(tokens)):\n",
    "        ngramseq=tokens[:j+1]\n",
    "        ip.append(ngramseq)\n",
    "        # print(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d381321-fe5a-4425-9f98-8cb282eb7385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "maxseqlen=max([len(seq) for seq in ip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8891474-e252-42f6-97f6-d487622eb634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "703e653f-306c-4589-bb89-1450366eacf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ip_seq=np.array(pad_sequences(ip,maxlen=maxseqlen,padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6601600a-c9c7-4d73-9748-867f76058f29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32382f3a-94fd-4d4c-80a5-31c9464d8c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X=ip_seq[:,:-1]\n",
    "y=ip_seq[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78feb68a-61d4-499e-85ca-5db32694ddf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   8,  10, ...,   1, 173, 174])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53403c40-bcad-4ece-bcf9-fbadf6478600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19e0d18f-fe71-4b63-83e3-2be20d1be6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y=np.array(to_categorical(y,num_classes=tot_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aa3d125-04c6-4843-aaff-37201ee8c417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdf449b8-adb3-4be0-b55b-6308f4b4e8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "647427f3-98ab-4746-935d-7b2d5a1782f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ae96667-ef67-423a-b536-75ac246b51a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "es=EarlyStopping(monitor='loss',patience=5,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b3a2287-c531-4ec9-8ce5-6e1a63cc8425",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 98, 100)           44700     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 447)               67497     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 262797 (1.00 MB)\n",
      "Trainable params: 262797 (1.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod=Sequential()\n",
    "mod.add(Embedding(tot_words,100,input_length=maxseqlen-1))\n",
    "mod.add(LSTM(150))\n",
    "mod.add(Dense(tot_words,activation='softmax'))\n",
    "mod.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4575594-0209-4be5-a8ba-afb628ea38cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 - 4s - loss: 6.0079 - accuracy: 0.0331 - 4s/epoch - 206ms/step\n",
      "Epoch 2/100\n",
      "20/20 - 2s - loss: 5.5438 - accuracy: 0.0541 - 2s/epoch - 81ms/step\n",
      "Epoch 3/100\n",
      "20/20 - 2s - loss: 5.4162 - accuracy: 0.0590 - 2s/epoch - 79ms/step\n",
      "Epoch 4/100\n",
      "20/20 - 2s - loss: 5.3857 - accuracy: 0.0590 - 2s/epoch - 78ms/step\n",
      "Epoch 5/100\n",
      "20/20 - 2s - loss: 5.3635 - accuracy: 0.0590 - 2s/epoch - 81ms/step\n",
      "Epoch 6/100\n",
      "20/20 - 2s - loss: 5.3291 - accuracy: 0.0816 - 2s/epoch - 81ms/step\n",
      "Epoch 7/100\n",
      "20/20 - 2s - loss: 5.2708 - accuracy: 0.0856 - 2s/epoch - 86ms/step\n",
      "Epoch 8/100\n",
      "20/20 - 2s - loss: 5.1903 - accuracy: 0.0921 - 2s/epoch - 78ms/step\n",
      "Epoch 9/100\n",
      "20/20 - 2s - loss: 5.0953 - accuracy: 0.0864 - 2s/epoch - 79ms/step\n",
      "Epoch 10/100\n",
      "20/20 - 2s - loss: 4.9826 - accuracy: 0.0994 - 2s/epoch - 98ms/step\n",
      "Epoch 11/100\n",
      "20/20 - 2s - loss: 4.8577 - accuracy: 0.1163 - 2s/epoch - 102ms/step\n",
      "Epoch 12/100\n",
      "20/20 - 2s - loss: 4.7244 - accuracy: 0.1430 - 2s/epoch - 88ms/step\n",
      "Epoch 13/100\n",
      "20/20 - 2s - loss: 4.5913 - accuracy: 0.1591 - 2s/epoch - 84ms/step\n",
      "Epoch 14/100\n",
      "20/20 - 2s - loss: 4.4511 - accuracy: 0.1817 - 2s/epoch - 83ms/step\n",
      "Epoch 15/100\n",
      "20/20 - 2s - loss: 4.3125 - accuracy: 0.1955 - 2s/epoch - 94ms/step\n",
      "Epoch 16/100\n",
      "20/20 - 2s - loss: 4.1822 - accuracy: 0.2141 - 2s/epoch - 92ms/step\n",
      "Epoch 17/100\n",
      "20/20 - 2s - loss: 4.0379 - accuracy: 0.2270 - 2s/epoch - 96ms/step\n",
      "Epoch 18/100\n",
      "20/20 - 2s - loss: 3.9065 - accuracy: 0.2399 - 2s/epoch - 92ms/step\n",
      "Epoch 19/100\n",
      "20/20 - 2s - loss: 3.7648 - accuracy: 0.2601 - 2s/epoch - 88ms/step\n",
      "Epoch 20/100\n",
      "20/20 - 2s - loss: 3.6283 - accuracy: 0.2706 - 2s/epoch - 90ms/step\n",
      "Epoch 21/100\n",
      "20/20 - 2s - loss: 3.4957 - accuracy: 0.2932 - 2s/epoch - 93ms/step\n",
      "Epoch 22/100\n",
      "20/20 - 2s - loss: 3.3559 - accuracy: 0.3239 - 2s/epoch - 89ms/step\n",
      "Epoch 23/100\n",
      "20/20 - 2s - loss: 3.2096 - accuracy: 0.3498 - 2s/epoch - 86ms/step\n",
      "Epoch 24/100\n",
      "20/20 - 2s - loss: 3.0724 - accuracy: 0.3667 - 2s/epoch - 84ms/step\n",
      "Epoch 25/100\n",
      "20/20 - 2s - loss: 2.9347 - accuracy: 0.3877 - 2s/epoch - 85ms/step\n",
      "Epoch 26/100\n",
      "20/20 - 2s - loss: 2.7923 - accuracy: 0.4225 - 2s/epoch - 84ms/step\n",
      "Epoch 27/100\n",
      "20/20 - 2s - loss: 2.6567 - accuracy: 0.4330 - 2s/epoch - 85ms/step\n",
      "Epoch 28/100\n",
      "20/20 - 2s - loss: 2.5259 - accuracy: 0.4733 - 2s/epoch - 86ms/step\n",
      "Epoch 29/100\n",
      "20/20 - 2s - loss: 2.3925 - accuracy: 0.5008 - 2s/epoch - 87ms/step\n",
      "Epoch 30/100\n",
      "20/20 - 2s - loss: 2.2695 - accuracy: 0.5339 - 2s/epoch - 83ms/step\n",
      "Epoch 31/100\n",
      "20/20 - 2s - loss: 2.1450 - accuracy: 0.5784 - 2s/epoch - 85ms/step\n",
      "Epoch 32/100\n",
      "20/20 - 2s - loss: 2.0286 - accuracy: 0.6171 - 2s/epoch - 84ms/step\n",
      "Epoch 33/100\n",
      "20/20 - 2s - loss: 1.9147 - accuracy: 0.6519 - 2s/epoch - 84ms/step\n",
      "Epoch 34/100\n",
      "20/20 - 2s - loss: 1.8039 - accuracy: 0.6785 - 2s/epoch - 84ms/step\n",
      "Epoch 35/100\n",
      "20/20 - 2s - loss: 1.7013 - accuracy: 0.6995 - 2s/epoch - 83ms/step\n",
      "Epoch 36/100\n",
      "20/20 - 2s - loss: 1.5968 - accuracy: 0.7326 - 2s/epoch - 81ms/step\n",
      "Epoch 37/100\n",
      "20/20 - 2s - loss: 1.5073 - accuracy: 0.7520 - 2s/epoch - 89ms/step\n",
      "Epoch 38/100\n",
      "20/20 - 2s - loss: 1.4149 - accuracy: 0.7827 - 2s/epoch - 85ms/step\n",
      "Epoch 39/100\n",
      "20/20 - 2s - loss: 1.3299 - accuracy: 0.8199 - 2s/epoch - 82ms/step\n",
      "Epoch 40/100\n",
      "20/20 - 2s - loss: 1.2488 - accuracy: 0.8441 - 2s/epoch - 82ms/step\n",
      "Epoch 41/100\n",
      "20/20 - 2s - loss: 1.1701 - accuracy: 0.8603 - 2s/epoch - 92ms/step\n",
      "Epoch 42/100\n",
      "20/20 - 2s - loss: 1.0959 - accuracy: 0.8821 - 2s/epoch - 94ms/step\n",
      "Epoch 43/100\n",
      "20/20 - 2s - loss: 1.0284 - accuracy: 0.8942 - 2s/epoch - 86ms/step\n",
      "Epoch 44/100\n",
      "20/20 - 2s - loss: 0.9680 - accuracy: 0.9055 - 2s/epoch - 84ms/step\n",
      "Epoch 45/100\n",
      "20/20 - 2s - loss: 0.9088 - accuracy: 0.9160 - 2s/epoch - 85ms/step\n",
      "Epoch 46/100\n",
      "20/20 - 2s - loss: 0.8563 - accuracy: 0.9225 - 2s/epoch - 85ms/step\n",
      "Epoch 47/100\n",
      "20/20 - 2s - loss: 0.8034 - accuracy: 0.9321 - 2s/epoch - 88ms/step\n",
      "Epoch 48/100\n",
      "20/20 - 2s - loss: 0.7580 - accuracy: 0.9338 - 2s/epoch - 86ms/step\n",
      "Epoch 49/100\n",
      "20/20 - 2s - loss: 0.7140 - accuracy: 0.9467 - 2s/epoch - 91ms/step\n",
      "Epoch 50/100\n",
      "20/20 - 2s - loss: 0.6703 - accuracy: 0.9418 - 2s/epoch - 93ms/step\n",
      "Epoch 51/100\n",
      "20/20 - 2s - loss: 0.6326 - accuracy: 0.9491 - 2s/epoch - 92ms/step\n",
      "Epoch 52/100\n",
      "20/20 - 2s - loss: 0.6007 - accuracy: 0.9556 - 2s/epoch - 92ms/step\n",
      "Epoch 53/100\n",
      "20/20 - 2s - loss: 0.5677 - accuracy: 0.9572 - 2s/epoch - 85ms/step\n",
      "Epoch 54/100\n",
      "20/20 - 2s - loss: 0.5342 - accuracy: 0.9620 - 2s/epoch - 84ms/step\n",
      "Epoch 55/100\n",
      "20/20 - 2s - loss: 0.5062 - accuracy: 0.9661 - 2s/epoch - 93ms/step\n",
      "Epoch 56/100\n",
      "20/20 - 2s - loss: 0.4741 - accuracy: 0.9685 - 2s/epoch - 90ms/step\n",
      "Epoch 57/100\n",
      "20/20 - 2s - loss: 0.4524 - accuracy: 0.9717 - 2s/epoch - 87ms/step\n",
      "Epoch 58/100\n",
      "20/20 - 2s - loss: 0.4322 - accuracy: 0.9742 - 2s/epoch - 87ms/step\n",
      "Epoch 59/100\n",
      "20/20 - 2s - loss: 0.4077 - accuracy: 0.9766 - 2s/epoch - 90ms/step\n",
      "Epoch 60/100\n",
      "20/20 - 2s - loss: 0.3866 - accuracy: 0.9742 - 2s/epoch - 96ms/step\n",
      "Epoch 61/100\n",
      "20/20 - 2s - loss: 0.3691 - accuracy: 0.9790 - 2s/epoch - 97ms/step\n",
      "Epoch 62/100\n",
      "20/20 - 2s - loss: 0.3513 - accuracy: 0.9822 - 2s/epoch - 94ms/step\n",
      "Epoch 63/100\n",
      "20/20 - 2s - loss: 0.3346 - accuracy: 0.9806 - 2s/epoch - 86ms/step\n",
      "Epoch 64/100\n",
      "20/20 - 2s - loss: 0.3187 - accuracy: 0.9830 - 2s/epoch - 95ms/step\n",
      "Epoch 65/100\n",
      "20/20 - 2s - loss: 0.3034 - accuracy: 0.9863 - 2s/epoch - 91ms/step\n",
      "Epoch 66/100\n",
      "20/20 - 2s - loss: 0.2907 - accuracy: 0.9838 - 2s/epoch - 90ms/step\n",
      "Epoch 67/100\n",
      "20/20 - 2s - loss: 0.2778 - accuracy: 0.9887 - 2s/epoch - 93ms/step\n",
      "Epoch 68/100\n",
      "20/20 - 2s - loss: 0.2653 - accuracy: 0.9863 - 2s/epoch - 87ms/step\n",
      "Epoch 69/100\n",
      "20/20 - 2s - loss: 0.2539 - accuracy: 0.9879 - 2s/epoch - 86ms/step\n",
      "Epoch 70/100\n",
      "20/20 - 2s - loss: 0.2438 - accuracy: 0.9879 - 2s/epoch - 87ms/step\n",
      "Epoch 71/100\n",
      "20/20 - 2s - loss: 0.2335 - accuracy: 0.9895 - 2s/epoch - 89ms/step\n",
      "Epoch 72/100\n",
      "20/20 - 2s - loss: 0.2249 - accuracy: 0.9887 - 2s/epoch - 95ms/step\n",
      "Epoch 73/100\n",
      "20/20 - 2s - loss: 0.2159 - accuracy: 0.9887 - 2s/epoch - 92ms/step\n",
      "Epoch 74/100\n",
      "20/20 - 2s - loss: 0.2063 - accuracy: 0.9895 - 2s/epoch - 88ms/step\n",
      "Epoch 75/100\n",
      "20/20 - 2s - loss: 0.1993 - accuracy: 0.9903 - 2s/epoch - 86ms/step\n",
      "Epoch 76/100\n",
      "20/20 - 2s - loss: 0.1919 - accuracy: 0.9895 - 2s/epoch - 84ms/step\n",
      "Epoch 77/100\n",
      "20/20 - 2s - loss: 0.1844 - accuracy: 0.9887 - 2s/epoch - 91ms/step\n",
      "Epoch 78/100\n",
      "20/20 - 2s - loss: 0.1783 - accuracy: 0.9887 - 2s/epoch - 92ms/step\n",
      "Epoch 79/100\n",
      "20/20 - 2s - loss: 0.1709 - accuracy: 0.9895 - 2s/epoch - 103ms/step\n",
      "Epoch 80/100\n",
      "20/20 - 2s - loss: 0.1661 - accuracy: 0.9903 - 2s/epoch - 115ms/step\n",
      "Epoch 81/100\n",
      "20/20 - 2s - loss: 0.1597 - accuracy: 0.9903 - 2s/epoch - 115ms/step\n",
      "Epoch 82/100\n",
      "20/20 - 2s - loss: 0.1550 - accuracy: 0.9887 - 2s/epoch - 89ms/step\n",
      "Epoch 83/100\n",
      "20/20 - 2s - loss: 0.1497 - accuracy: 0.9903 - 2s/epoch - 88ms/step\n",
      "Epoch 84/100\n",
      "20/20 - 2s - loss: 0.1448 - accuracy: 0.9887 - 2s/epoch - 91ms/step\n",
      "Epoch 85/100\n",
      "20/20 - 2s - loss: 0.1396 - accuracy: 0.9919 - 2s/epoch - 93ms/step\n",
      "Epoch 86/100\n",
      "20/20 - 2s - loss: 0.1360 - accuracy: 0.9895 - 2s/epoch - 92ms/step\n",
      "Epoch 87/100\n",
      "20/20 - 2s - loss: 0.1312 - accuracy: 0.9895 - 2s/epoch - 90ms/step\n",
      "Epoch 88/100\n",
      "20/20 - 2s - loss: 0.1279 - accuracy: 0.9887 - 2s/epoch - 88ms/step\n",
      "Epoch 89/100\n",
      "20/20 - 2s - loss: 0.1245 - accuracy: 0.9911 - 2s/epoch - 89ms/step\n",
      "Epoch 90/100\n",
      "20/20 - 2s - loss: 0.1203 - accuracy: 0.9911 - 2s/epoch - 96ms/step\n",
      "Epoch 91/100\n",
      "20/20 - 2s - loss: 0.1170 - accuracy: 0.9911 - 2s/epoch - 92ms/step\n",
      "Epoch 92/100\n",
      "20/20 - 2s - loss: 0.1143 - accuracy: 0.9911 - 2s/epoch - 89ms/step\n",
      "Epoch 93/100\n",
      "20/20 - 2s - loss: 0.1104 - accuracy: 0.9903 - 2s/epoch - 87ms/step\n",
      "Epoch 94/100\n",
      "20/20 - 2s - loss: 0.1078 - accuracy: 0.9919 - 2s/epoch - 88ms/step\n",
      "Epoch 95/100\n",
      "20/20 - 2s - loss: 0.1045 - accuracy: 0.9911 - 2s/epoch - 87ms/step\n",
      "Epoch 96/100\n",
      "20/20 - 2s - loss: 0.1025 - accuracy: 0.9911 - 2s/epoch - 86ms/step\n",
      "Epoch 97/100\n",
      "20/20 - 2s - loss: 0.0997 - accuracy: 0.9887 - 2s/epoch - 87ms/step\n",
      "Epoch 98/100\n",
      "20/20 - 2s - loss: 0.0971 - accuracy: 0.9887 - 2s/epoch - 91ms/step\n",
      "Epoch 99/100\n",
      "20/20 - 2s - loss: 0.0947 - accuracy: 0.9911 - 2s/epoch - 89ms/step\n",
      "Epoch 100/100\n",
      "20/20 - 2s - loss: 0.0925 - accuracy: 0.9903 - 2s/epoch - 87ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19b271667d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.fit(X,y,batch_size=64,epochs=100,verbose=2,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4853d239-b8dd-4484-813e-7174cfe327f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132]\n",
      "1/1 [==============================] - 0s 477ms/step\n",
      "[132, 13]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[132, 13, 230]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[132, 13, 230, 117]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[132, 13, 230, 117, 6]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[132, 13, 230, 117, 6, 19]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234, 19]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234, 19, 16]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234, 19, 16, 132]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234, 19, 16, 132, 13]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234, 19, 16, 132, 13, 87]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234, 19, 16, 132, 13, 87, 235]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234, 19, 16, 132, 13, 87, 235, 236]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234, 19, 16, 132, 13, 87, 235, 236, 11]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234, 19, 16, 132, 13, 87, 235, 236, 11, 45]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234, 19, 16, 132, 13, 87, 235, 236, 11, 45, 33]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[132, 13, 230, 117, 6, 19, 16, 32, 7, 4, 3, 231, 21, 35, 232, 233, 1, 48, 234, 19, 16, 132, 13, 87, 235, 236, 11, 45, 33, 15]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "CNNs are several types of neural networks used in deep learning each with its own strengths and applications convolutional neural networks cnns are particularly well suited for processing image data as\n"
     ]
    }
   ],
   "source": [
    "inputtxt=\"CNNs\"\n",
    "words=30\n",
    "\n",
    "for _ in range(words):\n",
    "    tokens=tk.texts_to_sequences([inputtxt])[0]\n",
    "    print(tokens)\n",
    "    tokens=pad_sequences([tokens], maxlen=maxseqlen-1, padding='pre')\n",
    "    pred=np.argmax(mod.predict(tokens),axis=-1)\n",
    "    op=\"\"\n",
    "    for wrd,idx in tk.word_index.items():\n",
    "        if idx==pred:\n",
    "            op=wrd\n",
    "            break\n",
    "    inputtxt+=\" \"+wrd\n",
    "\n",
    "print(inputtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b33577a0-e764-4ba7-97fe-10e5782e1e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhudi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "mod.save('100e.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "767b4911-a907-4492-9161-5ec6024562a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a27ad20f-d1f1-4afc-8e63-b6beb95868c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('tok.pkl', 'wb') as handle:\n",
    "    pickle.dump(tk, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3897700-13c1-42e6-85dc-9312147f9ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
